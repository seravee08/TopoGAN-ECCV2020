{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Include.ipynb\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class Net_block(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def Conv2d(params):\n",
    "        # params: input_channels, out_channels, kernel_size,\n",
    "        # stride, padding, bias\n",
    "        return nn.Conv2d(params[0], params[1], params[2],\n",
    "                         params[3], params[4], bias=params[5])\n",
    "    \n",
    "    @staticmethod\n",
    "    def SNConv2d(params):\n",
    "        # params: input_channels, out_channels, kernel_size,\n",
    "        # stride, padding, bias\n",
    "        return SpectralNorm(nn.Conv2d(params[0], params[1], params[2],\n",
    "                            params[3], params[4], bias=params[5]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def ConvT2d(params):\n",
    "        # params: input_channels, out_channels, kernel_size,\n",
    "        # stride, padding, bias\n",
    "        return nn.ConvTranspose2d(params[0], params[1],\n",
    "               params[2], params[3], params[4], bias=params[5])\n",
    "    \n",
    "    @staticmethod\n",
    "    def BN2d(params):\n",
    "        # params: num_features\n",
    "        return nn.BatchNorm2d(params[0])\n",
    "    \n",
    "    @staticmethod\n",
    "    def IN2d(params):\n",
    "        #params: num_features\n",
    "        return nn.InstanceNorm2d(params[0])\n",
    "    \n",
    "    @staticmethod\n",
    "    def Dropout(params):\n",
    "        #params: dropout ratio\n",
    "        return nn.Dropout(params[0])\n",
    "    \n",
    "    @staticmethod\n",
    "    def ReLU(params):\n",
    "        # params: inplace\n",
    "        return nn.ReLU(params[0])\n",
    "    \n",
    "    @staticmethod\n",
    "    def LeakyReLU(params):\n",
    "        # params: negative_slope, inplace\n",
    "        return nn.LeakyReLU(params[0], params[1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def Tanh(params):\n",
    "        return nn.Tanh()\n",
    "    \n",
    "    @staticmethod\n",
    "    def Sigmoid(params):\n",
    "        return nn.Sigmoid()\n",
    "    \n",
    "    @staticmethod\n",
    "    def AvgPool2d(params):\n",
    "        # params: kernel_size, stride, padding\n",
    "        return nn.AvgPool2d(params[0], params[1], params[2])\n",
    "    \n",
    "    @staticmethod\n",
    "    def Interpolate(params):\n",
    "        # params: scale_factor, mode\n",
    "        return Interpolate_(params[0], params[1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def RefPad2d(params):\n",
    "        return nn.ReflectionPad2d(params[0])\n",
    "    \n",
    "    @staticmethod\n",
    "    def RepPad2d(params):\n",
    "        return nn.ReplicationPad2d(params[0])\n",
    "\n",
    "    @staticmethod\n",
    "    def Identity(params):\n",
    "        return Identity_()\n",
    "    \n",
    "    @staticmethod\n",
    "    def Squeeze(params):\n",
    "        return Squeeze_()\n",
    "    \n",
    "    @staticmethod\n",
    "    def ResBlock2d(params):\n",
    "        # params: fin, fout, kernel_size, padding_type,\n",
    "        # norm_type, use_dropout, bias, addon_ratio, Conv2d type\n",
    "        return ResnetBlock(params[0], params[1], params[2], params[3],\n",
    "               params[4], params[5], params[6], params[7], params[8])\n",
    "    \n",
    "class Block_mapping(object):\n",
    "    \n",
    "    module_mapping = {\n",
    "        \"Conv2d\":       Net_block.Conv2d,\n",
    "        \"SNConv2d\":     Net_block.SNConv2d,\n",
    "        \"ConvT2d\":      Net_block.ConvT2d,\n",
    "        \"BN2d\":         Net_block.BN2d,\n",
    "        \"IN2d\":         Net_block.IN2d,\n",
    "        \"Dropout\":      Net_block.Dropout,\n",
    "        \"Relu\":         Net_block.ReLU,\n",
    "        \"LeakyRelu\":    Net_block.LeakyReLU,\n",
    "        \"Tanh\":         Net_block.Tanh,\n",
    "        \"Sigmoid\":      Net_block.Sigmoid,\n",
    "        \"AvgPool2d\":    Net_block.AvgPool2d,\n",
    "        \"Interpolate\":  Net_block.Interpolate,\n",
    "        \"RefPad2d\":     Net_block.RefPad2d,\n",
    "        \"RepPad2d\":     Net_block.RepPad2d,\n",
    "        \"Squeeze\":      Net_block.Squeeze,\n",
    "        \"ResBlock2d\":   Net_block.ResBlock2d,\n",
    "        \"None\":         Net_block.Identity\n",
    "    }\n",
    "\n",
    "class Squeeze_(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "class Identity_(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class Interpolate_(nn.Module):\n",
    "    def __init__(self, scale_factor, mode):\n",
    "        super(Interpolate_, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "    def forward(self, x):\n",
    "        return nn.functional.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "\n",
    "# ResnetBlock changese only C, NOT H or W\n",
    "class ResnetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, fin, fout, kernel_size, padding_type, norm_type, use_dropout, use_bias, addon_ratio, conv2d_type):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.addon_ratio = addon_ratio\n",
    "        self.conv_block, self.x_block = self.build_conv_block(fin, fout, kernel_size, padding_type,\n",
    "                                            norm_type, use_dropout, use_bias, conv2d_type)\n",
    "\n",
    "    def build_conv_block(self, fin, fout, kernel_size, padding_type, norm_type, use_dropout, use_bias, conv2d_type):\n",
    "        \"\"\"Construct a convolutional block.\n",
    "        Parameters:\n",
    "            fin(int)            -- the number of channels in the input\n",
    "            fout(int)           -- the number of channels in the output\n",
    "            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "            use_bias (bool)     -- if the conv layer uses bias or not\n",
    "            conv2d_type         -- to use normal conv2d or SNconv2d block\n",
    "        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n",
    "        \"\"\"\n",
    "        assert(conv2d_type == \"Conv2d\" or conv2d_type == \"SNConv2d\")\n",
    "        x_block    = []\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [Block_mapping.module_mapping[\"RefPad2d\"]([1])]\n",
    "            x_block    += [Block_mapping.module_mapping[\"RefPad2d\"]([1])]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [Block_mapping.module_mapping[\"RepPad2d\"]([1])]\n",
    "            x_block    += [Block_mapping.module_mapping[\"RepPad2d\"]([1])]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "            \n",
    "        if fin != fout:\n",
    "            x_block += [Block_mapping.module_mapping[conv2d_type]([fin, fout, kernel_size, 1, p, False]),\n",
    "                        Block_mapping.module_mapping[norm_type]([fout])]\n",
    "        else:\n",
    "            x_block += [Block_mapping.module_mapping[\"None\"]([])]\n",
    "\n",
    "        conv_block += [\n",
    "            Block_mapping.module_mapping[conv2d_type]([fin, fout, kernel_size, 1, p, use_bias]),\n",
    "            Block_mapping.module_mapping[norm_type]([fout]),\n",
    "            Block_mapping.module_mapping[\"Relu\"]([True])]\n",
    "        if use_dropout:\n",
    "            conv_block += [Block_mapping.module_mapping[\"Dropout\"]([0.5])]\n",
    "\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [Block_mapping.module_mapping[\"RefPad2d\"]([1])]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [Block_mapping.module_mapping[\"RepPad2d\"]([1])]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "        conv_block += [\n",
    "            Block_mapping.module_mapping[conv2d_type]([fout, fout, kernel_size, 1, p, use_bias]),\n",
    "            Block_mapping.module_mapping[norm_type]([fout])]\n",
    "\n",
    "        return nn.Sequential(*conv_block), nn.Sequential(*x_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.x_block(x) + self.conv_block(x) * self.addon_ratio\n",
    "        return out\n",
    "    \n",
    "# ===== Defination for Spectral Normalization\n",
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (v.norm() + eps)\n",
    "\n",
    "class SpectralNorm(nn.Module):\n",
    "    def __init__(self, module, name='weight', power_iterations=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        if not self._made_params():\n",
    "            self._make_params()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + \"_u\")\n",
    "        v = getattr(self.module, self.name + \"_v\")\n",
    "        w = getattr(self.module, self.name + \"_bar\")\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
    "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
    "\n",
    "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _made_params(self):\n",
    "        try:\n",
    "            u = getattr(self.module, self.name + \"_u\")\n",
    "            v = getattr(self.module, self.name + \"_v\")\n",
    "            w = getattr(self.module, self.name + \"_bar\")\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "\n",
    "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        u.data = l2normalize(u.data)\n",
    "        v.data = l2normalize(v.data)\n",
    "        w_bar = Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "        self.module.register_parameter(self.name + \"_u\", u)\n",
    "        self.module.register_parameter(self.name + \"_v\", v)\n",
    "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
    "\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
